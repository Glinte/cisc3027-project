{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e50b5faae3d08552",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T09:16:57.109938Z",
     "start_time": "2024-11-27T09:16:42.103123Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.transforms import v2\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import project.utils\n",
    "%reload project\n",
    "from project.data.luna_dataset import Luna16Dataset\n",
    "from project.models.vnet import VNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efccb0644ec8306d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VNet(num_classes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e08bf88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.4\n"
     ]
    }
   ],
   "source": [
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8256fd0556903c4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T09:16:57.135495Z",
     "start_time": "2024-11-27T09:16:57.113945Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38e3bf47a20b1500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VNet(\n",
       "  (input_block): VNet_input_block(\n",
       "    (conv1): Conv3d(1, 16, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n",
       "    (bn1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu1): PReLU(num_parameters=16)\n",
       "  )\n",
       "  (down_block1): VNet_down_block(\n",
       "    (down_conv): Conv3d(16, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "    (bn1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu1): PReLU(num_parameters=32)\n",
       "    (convs): Sequential(\n",
       "      (0): Conv_in_stage(\n",
       "        (conv1): Conv3d(32, 32, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n",
       "        (bn1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (1): Conv_in_stage(\n",
       "        (conv1): Conv3d(32, 32, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n",
       "        (bn1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): PReLU(num_parameters=1)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down_block2): VNet_down_block(\n",
       "    (down_conv): Conv3d(32, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "    (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu1): PReLU(num_parameters=64)\n",
       "    (convs): Sequential(\n",
       "      (0): Conv_in_stage(\n",
       "        (conv1): Conv3d(64, 64, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n",
       "        (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (1): Conv_in_stage(\n",
       "        (conv1): Conv3d(64, 64, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n",
       "        (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (2): Conv_in_stage(\n",
       "        (conv1): Conv3d(64, 64, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n",
       "        (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): PReLU(num_parameters=1)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down_block3): VNet_down_block(\n",
       "    (down_conv): Conv3d(64, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "    (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu1): PReLU(num_parameters=128)\n",
       "    (convs): Sequential(\n",
       "      (0): Conv_in_stage(\n",
       "        (conv1): Conv3d(128, 128, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n",
       "        (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (1): Conv_in_stage(\n",
       "        (conv1): Conv3d(128, 128, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n",
       "        (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (2): Conv_in_stage(\n",
       "        (conv1): Conv3d(128, 128, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n",
       "        (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): PReLU(num_parameters=1)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down_block4): VNet_down_block(\n",
       "    (down_conv): Conv3d(128, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "    (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu1): PReLU(num_parameters=256)\n",
       "    (convs): Sequential(\n",
       "      (0): Conv_in_stage(\n",
       "        (conv1): Conv3d(256, 256, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n",
       "        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (1): Conv_in_stage(\n",
       "        (conv1): Conv3d(256, 256, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n",
       "        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (2): Conv_in_stage(\n",
       "        (conv1): Conv3d(256, 256, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n",
       "        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): PReLU(num_parameters=1)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up_block1): VNet_up_block(\n",
       "    (up_conv): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "    (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu1): PReLU(num_parameters=128)\n",
       "    (convs): Sequential(\n",
       "      (0): Conv_in_stage(\n",
       "        (conv1): Conv3d(256, 256, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n",
       "        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (1): Conv_in_stage(\n",
       "        (conv1): Conv3d(256, 256, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n",
       "        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (2): Conv_in_stage(\n",
       "        (conv1): Conv3d(256, 256, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n",
       "        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): PReLU(num_parameters=1)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up_block2): VNet_up_block(\n",
       "    (up_conv): ConvTranspose3d(256, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "    (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu1): PReLU(num_parameters=64)\n",
       "    (convs): Sequential(\n",
       "      (0): Conv_in_stage(\n",
       "        (conv1): Conv3d(128, 128, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n",
       "        (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (1): Conv_in_stage(\n",
       "        (conv1): Conv3d(128, 128, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n",
       "        (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (2): Conv_in_stage(\n",
       "        (conv1): Conv3d(128, 128, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n",
       "        (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): PReLU(num_parameters=1)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up_block3): VNet_up_block(\n",
       "    (up_conv): ConvTranspose3d(128, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "    (bn1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu1): PReLU(num_parameters=32)\n",
       "    (convs): Sequential(\n",
       "      (0): Conv_in_stage(\n",
       "        (conv1): Conv3d(64, 64, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n",
       "        (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (1): Conv_in_stage(\n",
       "        (conv1): Conv3d(64, 64, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n",
       "        (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): PReLU(num_parameters=1)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up_block4): VNet_up_block(\n",
       "    (up_conv): ConvTranspose3d(64, 16, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "    (bn1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu1): PReLU(num_parameters=16)\n",
       "    (convs): Sequential(\n",
       "      (0): Conv_in_stage(\n",
       "        (conv1): Conv3d(32, 32, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n",
       "        (bn1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): PReLU(num_parameters=1)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output_block): VNet_output_block(\n",
       "    (conv1): Conv3d(32, 1, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n",
       "    (bn1): BatchNorm3d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv3d(1, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "    (relu1): PReLU(num_parameters=1)\n",
       "    (softmax): Softmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da5866c3e691e11b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T09:20:14.228207Z",
     "start_time": "2024-11-27T09:20:14.182377Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torchvision.transforms import v2\n",
    "from torchvision import tv_tensors\n",
    "from project.config import PROJECT_ROOT\n",
    "\n",
    "class PadDepthTransform(nn.Module):\n",
    "    \"\"\"Pad the depth dimension of the input tensor to be divisible by 16.\"\"\"\n",
    "    def forward(self, img, mask) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Check if the number of depth dimensions is odd\n",
    "        if img.shape[1] % 16 != 0:\n",
    "            # Create a zero-filled padding with the same height and width\n",
    "            padding = torch.zeros(1, 16 - img.shape[1] % 16, *img.shape[2:], device=img.device, dtype=img.dtype)\n",
    "            # Concatenate the padding to the tensor\n",
    "            img = torch.cat([img, padding], dim=1)\n",
    "            mask = torch.cat([mask, padding], dim=1)\n",
    "        return tv_tensors.Image(img), tv_tensors.Mask(mask)\n",
    "    \n",
    "class Normalize3D(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super().__init__()\n",
    "        self.mean = torch.tensor(mean)\n",
    "        self.std = torch.tensor(std)\n",
    "        \n",
    "    def forward(self, img, mask) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        img = (img - self.mean) / self.std\n",
    "        return img, mask\n",
    "    \n",
    "class Resize3d(nn.Module):\n",
    "    def __init__(self, size: tuple[int, int, int]):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        \n",
    "    def forward(self, img: torch.Tensor, mask: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        img = nn.functional.interpolate(img.unsqueeze(0).float(), size=self.size, mode=\"trilinear\", align_corners=False).squeeze(0).long()\n",
    "        mask = nn.functional.interpolate(mask.unsqueeze(0).float(), size=self.size, mode=\"nearest\").squeeze(0).long()\n",
    "        return img, mask\n",
    "\n",
    "transforms = v2.Compose([\n",
    "    Resize3d(size=(224, 224, 224)),\n",
    "    PadDepthTransform(),\n",
    "    v2.ToDtype({tv_tensors.Image: torch.float32, tv_tensors.Mask: torch.int64, \"others\": None}, scale=True),\n",
    "    Normalize3D(mean=-0.023, std=0.026),\n",
    "])\n",
    "\n",
    "luna16 = Luna16Dataset(root=PROJECT_ROOT / \"data/luna16\", transforms=transforms, train=True)\n",
    "luna16_base = Luna16Dataset(root=PROJECT_ROOT / \"data/luna16\", transforms=None, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f46e9fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 256, 256])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resize = Resize3d((256, 256, 256))\n",
    "resize(*luna16_base[0])[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7d5fd35699d1861",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T09:20:15.011885Z",
     "start_time": "2024-11-27T09:20:14.873682Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0.8846, 0.8846, 0.8846,  ..., 0.8846, 0.8846, 0.8846],\n",
       "           [0.8846, 0.8846, 0.8846,  ..., 0.8846, 0.8846, 0.8846],\n",
       "           [0.8846, 0.8846, 0.8846,  ..., 0.8846, 0.8846, 0.8846],\n",
       "           ...,\n",
       "           [0.8846, 0.8846, 0.8846,  ..., 0.8846, 0.8846, 0.8846],\n",
       "           [0.8846, 0.8846, 0.8846,  ..., 0.8846, 0.8846, 0.8846],\n",
       "           [0.8846, 0.8846, 0.8846,  ..., 0.8846, 0.8846, 0.8846]],\n",
       " \n",
       "          [[0.8846, 0.8846, 0.8846,  ..., 0.8846, 0.8846, 0.8846],\n",
       "           [0.8846, 0.8846, 0.8846,  ..., 0.8846, 0.8846, 0.8846],\n",
       "           [0.8846, 0.8846, 0.8846,  ..., 0.8846, 0.8846, 0.8846],\n",
       "           ...,\n",
       "           [0.8846, 0.8846, 0.8846,  ..., 0.8846, 0.8846, 0.8846],\n",
       "           [0.8846, 0.8846, 0.8846,  ..., 0.8846, 0.8846, 0.8846],\n",
       "           [0.8846, 0.8846, 0.8846,  ..., 0.8846, 0.8846, 0.8846]],\n",
       " \n",
       "          [[0.8846, 0.8846, 0.8846,  ..., 0.8846, 0.8846, 0.8846],\n",
       "           [0.8846, 0.8846, 0.8846,  ..., 0.8846, 0.8846, 0.8846],\n",
       "           [0.8846, 0.8846, 0.8846,  ..., 0.8846, 0.8846, 0.8846],\n",
       "           ...,\n",
       "           [0.8846, 0.8846, 0.8846,  ..., 0.8846, 0.8846, 0.8846],\n",
       "           [0.8846, 0.8846, 0.8846,  ..., 0.8846, 0.8846, 0.8846],\n",
       "           [0.8846, 0.8846, 0.8846,  ..., 0.8846, 0.8846, 0.8846]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0.8846, 0.8846, 0.8846,  ..., 0.8846, 0.8846, 0.8846],\n",
       "           [0.8846, 0.8846, 0.8846,  ..., 0.8846, 0.8846, 0.8846],\n",
       "           [0.8846, 0.8846, 0.8846,  ..., 0.8846, 0.8846, 0.8846],\n",
       "           ...,\n",
       "           [0.8846, 0.8846, 0.8846,  ..., 0.8846, 0.8846, 0.8846],\n",
       "           [0.8846, 0.8846, 0.8846,  ..., 0.8846, 0.8846, 0.8846],\n",
       "           [0.8846, 0.8846, 0.8846,  ..., 0.8846, 0.8846, 0.8846]],\n",
       " \n",
       "          [[0.8846, 0.8846, 0.8846,  ..., 0.8846, 0.8846, 0.8846],\n",
       "           [0.8846, 0.8846, 0.8846,  ..., 0.8846, 0.8846, 0.8846],\n",
       "           [0.8846, 0.8846, 0.8846,  ..., 0.8846, 0.8846, 0.8846],\n",
       "           ...,\n",
       "           [0.8846, 0.8846, 0.8846,  ..., 0.8846, 0.8846, 0.8846],\n",
       "           [0.8846, 0.8846, 0.8846,  ..., 0.8846, 0.8846, 0.8846],\n",
       "           [0.8846, 0.8846, 0.8846,  ..., 0.8846, 0.8846, 0.8846]],\n",
       " \n",
       "          [[0.8846, 0.8846, 0.8846,  ..., 0.8846, 0.8846, 0.8846],\n",
       "           [0.8846, 0.8846, 0.8846,  ..., 0.8846, 0.8846, 0.8846],\n",
       "           [0.8846, 0.8846, 0.8846,  ..., 0.8846, 0.8846, 0.8846],\n",
       "           ...,\n",
       "           [0.8846, 0.8846, 0.8846,  ..., 0.8846, 0.8846, 0.8846],\n",
       "           [0.8846, 0.8846, 0.8846,  ..., 0.8846, 0.8846, 0.8846],\n",
       "           [0.8846, 0.8846, 0.8846,  ..., 0.8846, 0.8846, 0.8846]]]]),\n",
       " Mask([[[[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transforms(*luna16_base[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5caef7402b15c5b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T09:20:16.091456Z",
     "start_time": "2024-11-27T09:20:16.087232Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "712"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(luna16.masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "266e7527872f1060",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T09:20:17.683145Z",
     "start_time": "2024-11-27T09:20:17.410295Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 224, 224, 224]), torch.Size([1, 224, 224, 224]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "luna16[0][0].shape, luna16[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "345c2721b92e2910",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T09:20:19.137440Z",
     "start_time": "2024-11-27T09:20:19.134873Z"
    }
   },
   "outputs": [],
   "source": [
    "luna_train_loader = DataLoader(luna16, batch_size=1, shuffle=False)  # Batch size has to be 1 because each image has different depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26d1f09e574496e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T09:20:41.951347Z",
     "start_time": "2024-11-27T09:20:19.781129Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0])\n",
      "tensor([  0, 255])\n",
      "tensor([  0, 255])\n",
      "tensor([  0, 255])\n",
      "tensor([0])\n",
      "tensor([  0, 255])\n",
      "tensor([0])\n"
     ]
    }
   ],
   "source": [
    "for i, (data, target) in enumerate(luna_train_loader):\n",
    "    # print(data.shape, data.dtype)\n",
    "    # print(target.shape, target.dtype)\n",
    "    print(target.unique())\n",
    "    if i > 5:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b418988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# means = []\n",
    "# stds = []\n",
    "# for i, (data, target) in enumerate(luna_train_loader):\n",
    "#     mean = data.mean()\n",
    "#     std = data.std()\n",
    "#     means.append(mean)\n",
    "#     stds.append(std)\n",
    "    \n",
    "# print(f\"Mean: {torch.stack(means).mean()}\")\n",
    "# print(f\"Std: {torch.stack(stds).mean()}\")\n",
    "\n",
    "# output\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77a95260bbc638c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T09:17:16.739191400Z",
     "start_time": "2024-11-27T09:16:32.638460Z"
    }
   },
   "outputs": [],
   "source": [
    "from project.models.vnet import VNet\n",
    "from project.models.unet3d import UNet3D\n",
    "# import segmentation_models_3D as sm\n",
    "\n",
    "# model = VNet(num_classes=1)\n",
    "model = UNet3D(n_channels=1, n_classes=1)\n",
    "# model = sm.FPN(\n",
    "#     'densenet121',\n",
    "#     classes=1,\n",
    "#     activation='sigmoid'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45eff2184a4b5d30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T09:00:34.450553Z",
     "start_time": "2024-11-27T09:00:34.446245Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12946785"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of parameters\n",
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3548ea99",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8703874940603b70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T09:02:46.202241Z",
     "start_time": "2024-11-27T09:02:45.941410Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: papetoast (papetoast-org1). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\user\\cisc3027-project\\notebooks\\wandb\\run-20241127_183554-n8x2ixoy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/papetoast-org1/luna16/runs/n8x2ixoy' target=\"_blank\">ethereal-bush-8</a></strong> to <a href='https://wandb.ai/papetoast-org1/luna16' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/papetoast-org1/luna16' target=\"_blank\">https://wandb.ai/papetoast-org1/luna16</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/papetoast-org1/luna16/runs/n8x2ixoy' target=\"_blank\">https://wandb.ai/papetoast-org1/luna16/runs/n8x2ixoy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Loss: 0.8855414986610413\n",
      "Epoch: 0, Batch: 1, Loss: 0.798898458480835\n",
      "Epoch: 0, Batch: 2, Loss: 0.7426631450653076\n",
      "Epoch: 0, Batch: 3, Loss: 0.6973922848701477\n",
      "Epoch: 0, Batch: 4, Loss: 0.674547553062439\n",
      "Epoch: 0, Batch: 5, Loss: 0.6578269600868225\n",
      "Epoch: 0, Batch: 6, Loss: 0.6408712267875671\n",
      "Epoch: 0, Batch: 7, Loss: 0.6274880170822144\n",
      "Epoch: 0, Batch: 8, Loss: 0.6131066083908081\n",
      "Epoch: 0, Batch: 9, Loss: 0.5927512049674988\n",
      "Epoch: 0, Batch: 10, Loss: 0.5830245614051819\n",
      "Epoch: 0, Batch: 11, Loss: 0.5767033100128174\n",
      "Epoch: 0, Batch: 12, Loss: 0.5765429735183716\n",
      "Epoch: 0, Batch: 13, Loss: 0.5502112507820129\n",
      "Epoch: 0, Batch: 14, Loss: 0.5404123067855835\n",
      "Epoch: 0, Batch: 15, Loss: 0.5301836133003235\n",
      "Epoch: 0, Batch: 16, Loss: 0.5238696336746216\n",
      "Epoch: 0, Batch: 17, Loss: 0.5150948166847229\n",
      "Epoch: 0, Batch: 18, Loss: 0.5092343091964722\n",
      "Epoch: 0, Batch: 19, Loss: 0.5143136978149414\n",
      "Epoch: 0, Batch: 20, Loss: 0.5264184474945068\n",
      "Epoch: 0, Batch: 21, Loss: 0.49556928873062134\n",
      "Epoch: 0, Batch: 22, Loss: 0.4953083097934723\n",
      "Epoch: 0, Batch: 23, Loss: 0.48595499992370605\n",
      "Epoch: 0, Batch: 24, Loss: 0.4781827926635742\n",
      "Epoch: 0, Batch: 25, Loss: 0.48641619086265564\n",
      "Epoch: 0, Batch: 26, Loss: 0.4691937565803528\n",
      "Epoch: 0, Batch: 27, Loss: 0.48225054144859314\n",
      "Epoch: 0, Batch: 28, Loss: 0.4625222384929657\n",
      "Epoch: 0, Batch: 29, Loss: 0.4655601680278778\n",
      "Epoch: 0, Batch: 30, Loss: 0.46770840883255005\n",
      "Epoch: 0, Batch: 31, Loss: 0.4713159501552582\n",
      "Epoch: 0, Batch: 32, Loss: 0.4455524682998657\n",
      "Epoch: 0, Batch: 33, Loss: 0.44183626770973206\n",
      "Epoch: 0, Batch: 34, Loss: 0.43857818841934204\n",
      "Epoch: 0, Batch: 35, Loss: 0.44745391607284546\n",
      "Epoch: 0, Batch: 36, Loss: 0.4317738711833954\n",
      "Epoch: 0, Batch: 37, Loss: 0.43925681710243225\n",
      "Epoch: 0, Batch: 38, Loss: 0.43527552485466003\n",
      "Epoch: 0, Batch: 39, Loss: 0.42596736550331116\n",
      "Epoch: 0, Batch: 40, Loss: 0.42293503880500793\n",
      "Epoch: 0, Batch: 41, Loss: 0.41773584485054016\n",
      "Epoch: 0, Batch: 42, Loss: 0.41353651881217957\n",
      "Epoch: 0, Batch: 43, Loss: 0.4109836518764496\n",
      "Epoch: 0, Batch: 44, Loss: 0.40799760818481445\n",
      "Epoch: 0, Batch: 45, Loss: 0.40967419743537903\n",
      "Epoch: 0, Batch: 46, Loss: 0.4007023870944977\n",
      "Epoch: 0, Batch: 47, Loss: 0.40302717685699463\n",
      "Epoch: 0, Batch: 48, Loss: 0.39144009351730347\n",
      "Epoch: 0, Batch: 49, Loss: 0.38922423124313354\n",
      "Epoch: 0, Batch: 50, Loss: 0.39419373869895935\n",
      "Epoch: 0, Batch: 51, Loss: 0.386405348777771\n",
      "Epoch: 0, Batch: 52, Loss: 0.37906739115715027\n",
      "Epoch: 0, Batch: 53, Loss: 0.37616127729415894\n",
      "Epoch: 0, Batch: 54, Loss: 0.38707709312438965\n",
      "Epoch: 0, Batch: 55, Loss: 0.37201815843582153\n",
      "Epoch: 0, Batch: 56, Loss: 0.3796166181564331\n",
      "Epoch: 0, Batch: 57, Loss: 0.3708841800689697\n",
      "Epoch: 0, Batch: 58, Loss: 0.3656036853790283\n",
      "Epoch: 0, Batch: 59, Loss: 0.36303281784057617\n",
      "Epoch: 0, Batch: 60, Loss: 0.356113463640213\n",
      "Epoch: 0, Batch: 61, Loss: 0.3560892641544342\n",
      "Epoch: 0, Batch: 62, Loss: 0.3746410012245178\n",
      "Epoch: 0, Batch: 63, Loss: 0.3503345251083374\n",
      "Epoch: 0, Batch: 64, Loss: 0.3456819951534271\n",
      "Epoch: 0, Batch: 65, Loss: 0.3443276286125183\n",
      "Epoch: 0, Batch: 66, Loss: 0.3412225842475891\n",
      "Epoch: 0, Batch: 67, Loss: 0.35087937116622925\n",
      "Epoch: 0, Batch: 68, Loss: 0.34706762433052063\n",
      "Epoch: 0, Batch: 69, Loss: 0.41946011781692505\n",
      "Epoch: 0, Batch: 70, Loss: 0.3389334976673126\n",
      "Epoch: 0, Batch: 71, Loss: 0.3525015413761139\n",
      "Epoch: 0, Batch: 72, Loss: 0.33009374141693115\n",
      "Epoch: 0, Batch: 73, Loss: 0.32345834374427795\n",
      "Epoch: 0, Batch: 74, Loss: 0.3236517906188965\n",
      "Epoch: 0, Batch: 75, Loss: 0.3230400085449219\n",
      "Epoch: 0, Batch: 76, Loss: 0.31857019662857056\n",
      "Epoch: 0, Batch: 77, Loss: 0.3138640224933624\n",
      "Epoch: 0, Batch: 78, Loss: 0.31818467378616333\n",
      "Epoch: 0, Batch: 79, Loss: 0.31322890520095825\n",
      "Epoch: 0, Batch: 80, Loss: 0.3473978340625763\n",
      "Epoch: 0, Batch: 81, Loss: 0.3288809359073639\n",
      "Epoch: 0, Batch: 82, Loss: 0.3069019019603729\n",
      "Epoch: 0, Batch: 83, Loss: 0.3047579228878021\n",
      "Epoch: 0, Batch: 84, Loss: 0.30335894227027893\n",
      "Epoch: 0, Batch: 85, Loss: 0.29958751797676086\n",
      "Epoch: 0, Batch: 86, Loss: 0.3298766613006592\n",
      "Epoch: 0, Batch: 87, Loss: 0.29089391231536865\n",
      "Epoch: 0, Batch: 88, Loss: 0.2959040105342865\n",
      "Epoch: 0, Batch: 89, Loss: 0.2863183617591858\n",
      "Epoch: 0, Batch: 90, Loss: 0.2949458658695221\n",
      "Epoch: 0, Batch: 91, Loss: 0.30459946393966675\n",
      "Epoch: 0, Batch: 92, Loss: 0.2814633250236511\n",
      "Epoch: 0, Batch: 93, Loss: 0.27774298191070557\n",
      "Epoch: 0, Batch: 94, Loss: 0.27578461170196533\n",
      "Epoch: 0, Batch: 95, Loss: 0.2856546938419342\n",
      "Epoch: 0, Batch: 96, Loss: 0.2758738100528717\n",
      "Epoch: 0, Batch: 97, Loss: 0.27605023980140686\n",
      "Epoch: 0, Batch: 98, Loss: 0.28806954622268677\n",
      "Epoch: 0, Batch: 99, Loss: 0.273882657289505\n",
      "Epoch: 0, Batch: 100, Loss: 0.26296448707580566\n",
      "Epoch: 0, Batch: 101, Loss: 0.2609045207500458\n",
      "Epoch: 0, Batch: 102, Loss: 0.2586786150932312\n",
      "Epoch: 0, Batch: 103, Loss: 0.26135140657424927\n",
      "Epoch: 0, Batch: 104, Loss: 0.25445112586021423\n",
      "Epoch: 0, Batch: 105, Loss: 0.25248056650161743\n",
      "Epoch: 0, Batch: 106, Loss: 0.25039076805114746\n",
      "Epoch: 0, Batch: 107, Loss: 0.24836675822734833\n",
      "Epoch: 0, Batch: 108, Loss: 0.2462829351425171\n",
      "Epoch: 0, Batch: 109, Loss: 0.25398632884025574\n",
      "Epoch: 0, Batch: 110, Loss: 0.24592959880828857\n",
      "Epoch: 0, Batch: 111, Loss: 0.24294431507587433\n",
      "Epoch: 0, Batch: 112, Loss: 0.24248449504375458\n",
      "Epoch: 0, Batch: 113, Loss: 0.23694980144500732\n",
      "Epoch: 0, Batch: 114, Loss: 0.2943732440471649\n",
      "Epoch: 0, Batch: 115, Loss: 0.2318781316280365\n",
      "Epoch: 0, Batch: 116, Loss: 0.34023594856262207\n",
      "Epoch: 0, Batch: 117, Loss: 0.32139551639556885\n",
      "Epoch: 0, Batch: 118, Loss: 0.2364356964826584\n",
      "Epoch: 0, Batch: 119, Loss: 0.22998376190662384\n",
      "Epoch: 0, Batch: 120, Loss: 0.23838253319263458\n",
      "Epoch: 0, Batch: 121, Loss: 0.2218264490365982\n",
      "Epoch: 0, Batch: 122, Loss: 0.21958479285240173\n",
      "Epoch: 0, Batch: 123, Loss: 0.217253178358078\n",
      "Epoch: 0, Batch: 124, Loss: 0.2192927598953247\n",
      "Epoch: 0, Batch: 125, Loss: 0.21341587603092194\n",
      "Epoch: 0, Batch: 126, Loss: 0.21180905401706696\n",
      "Epoch: 0, Batch: 127, Loss: 0.21744897961616516\n",
      "Epoch: 0, Batch: 128, Loss: 0.2717486023902893\n",
      "Epoch: 0, Batch: 129, Loss: 0.20670001208782196\n",
      "Epoch: 0, Batch: 130, Loss: 0.20478834211826324\n",
      "Epoch: 0, Batch: 131, Loss: 0.2265826314687729\n",
      "Epoch: 0, Batch: 132, Loss: 0.21091924607753754\n",
      "Epoch: 0, Batch: 133, Loss: 0.19985851645469666\n",
      "Epoch: 0, Batch: 134, Loss: 0.2616369128227234\n",
      "Epoch: 0, Batch: 135, Loss: 0.2080169916152954\n",
      "Epoch: 0, Batch: 136, Loss: 0.2262095957994461\n",
      "Epoch: 0, Batch: 137, Loss: 0.1946530044078827\n",
      "Epoch: 0, Batch: 138, Loss: 0.1988493651151657\n",
      "Epoch: 0, Batch: 139, Loss: 0.2214842438697815\n",
      "Epoch: 0, Batch: 140, Loss: 0.19759996235370636\n",
      "Epoch: 0, Batch: 141, Loss: 0.20322394371032715\n",
      "Epoch: 0, Batch: 142, Loss: 0.23072028160095215\n",
      "Epoch: 0, Batch: 143, Loss: 0.1835702508687973\n",
      "Epoch: 0, Batch: 144, Loss: 0.47044458985328674\n",
      "Epoch: 0, Batch: 145, Loss: 0.18478763103485107\n",
      "Epoch: 0, Batch: 146, Loss: 0.3075301945209503\n",
      "Epoch: 0, Batch: 147, Loss: 0.18938809633255005\n",
      "Epoch: 0, Batch: 148, Loss: 0.19388721883296967\n",
      "Epoch: 0, Batch: 149, Loss: 0.20882169902324677\n",
      "Epoch: 0, Batch: 150, Loss: 0.18477998673915863\n",
      "Epoch: 0, Batch: 151, Loss: 0.1805163025856018\n",
      "Epoch: 0, Batch: 152, Loss: 0.23026950657367706\n",
      "Epoch: 0, Batch: 153, Loss: 0.20918217301368713\n",
      "Epoch: 0, Batch: 154, Loss: 0.17188259959220886\n",
      "Epoch: 0, Batch: 155, Loss: 0.173191636800766\n",
      "Epoch: 0, Batch: 156, Loss: 0.1678735613822937\n",
      "Epoch: 0, Batch: 157, Loss: 0.16787318885326385\n",
      "Epoch: 0, Batch: 158, Loss: 0.16860130429267883\n",
      "Epoch: 0, Batch: 159, Loss: 0.17764581739902496\n",
      "Epoch: 0, Batch: 160, Loss: 0.26931044459342957\n",
      "Epoch: 0, Batch: 161, Loss: 0.1824990063905716\n",
      "Epoch: 0, Batch: 162, Loss: 0.2773115038871765\n",
      "Epoch: 0, Batch: 163, Loss: 0.23263905942440033\n",
      "Epoch: 0, Batch: 164, Loss: 0.16033515334129333\n",
      "Epoch: 0, Batch: 165, Loss: 0.1615688055753708\n",
      "Epoch: 0, Batch: 166, Loss: 0.18707478046417236\n",
      "Epoch: 0, Batch: 167, Loss: 0.16041600704193115\n",
      "Epoch: 0, Batch: 168, Loss: 0.16016879677772522\n",
      "Epoch: 0, Batch: 169, Loss: 0.21795813739299774\n",
      "Epoch: 0, Batch: 170, Loss: 0.15188106894493103\n",
      "Epoch: 0, Batch: 171, Loss: 0.15746888518333435\n",
      "Epoch: 0, Batch: 172, Loss: 0.1496751606464386\n",
      "Epoch: 0, Batch: 173, Loss: 0.22111456096172333\n",
      "Epoch: 0, Batch: 174, Loss: 0.14943258464336395\n",
      "Epoch: 0, Batch: 175, Loss: 0.16218428313732147\n",
      "Epoch: 0, Batch: 176, Loss: 0.15881159901618958\n",
      "Epoch: 0, Batch: 177, Loss: 0.1479634940624237\n",
      "Epoch: 0, Batch: 178, Loss: 0.44067785143852234\n",
      "Epoch: 0, Batch: 179, Loss: 0.14271856844425201\n",
      "Epoch: 0, Batch: 180, Loss: 0.1425926238298416\n",
      "Epoch: 0, Batch: 181, Loss: 0.14226436614990234\n",
      "Epoch: 0, Batch: 182, Loss: 0.14143681526184082\n",
      "Epoch: 0, Batch: 183, Loss: 0.2199137657880783\n",
      "Epoch: 0, Batch: 184, Loss: 0.17477299273014069\n",
      "Epoch: 0, Batch: 185, Loss: 0.141042560338974\n",
      "Epoch: 0, Batch: 186, Loss: 0.19446323812007904\n",
      "Epoch: 0, Batch: 187, Loss: 0.13868339359760284\n",
      "Epoch: 0, Batch: 188, Loss: 0.13442733883857727\n",
      "Epoch: 0, Batch: 189, Loss: 0.14056548476219177\n",
      "Epoch: 0, Batch: 190, Loss: 0.1362663358449936\n",
      "Epoch: 0, Batch: 191, Loss: 0.20410165190696716\n",
      "Epoch: 0, Batch: 192, Loss: 0.13653360307216644\n",
      "Epoch: 0, Batch: 193, Loss: 0.26025646924972534\n",
      "Epoch: 0, Batch: 194, Loss: 0.12809374928474426\n",
      "Epoch: 0, Batch: 195, Loss: 0.12788020074367523\n",
      "Epoch: 0, Batch: 196, Loss: 0.1273275911808014\n",
      "Epoch: 0, Batch: 197, Loss: 0.14362312853336334\n",
      "Epoch: 0, Batch: 198, Loss: 0.26921793818473816\n",
      "Epoch: 0, Batch: 199, Loss: 0.12526580691337585\n",
      "Epoch: 0, Batch: 200, Loss: 0.12495767325162888\n",
      "Epoch: 0, Batch: 201, Loss: 0.18377554416656494\n",
      "Epoch: 0, Batch: 202, Loss: 0.12668287754058838\n",
      "Epoch: 0, Batch: 203, Loss: 0.16422739624977112\n",
      "Epoch: 0, Batch: 204, Loss: 0.1441420018672943\n",
      "Epoch: 0, Batch: 205, Loss: 0.12451823800802231\n",
      "Epoch: 0, Batch: 206, Loss: 0.11776881664991379\n",
      "Epoch: 0, Batch: 207, Loss: 0.13398216664791107\n",
      "Epoch: 0, Batch: 208, Loss: 0.11979695409536362\n",
      "Epoch: 0, Batch: 209, Loss: 0.11790008842945099\n",
      "Epoch: 0, Batch: 210, Loss: 0.12508006393909454\n",
      "Epoch: 0, Batch: 211, Loss: 0.12519657611846924\n",
      "Epoch: 0, Batch: 212, Loss: 0.11178506910800934\n",
      "Epoch: 0, Batch: 213, Loss: 0.11101894825696945\n",
      "Epoch: 0, Batch: 214, Loss: 0.12579548358917236\n",
      "Epoch: 0, Batch: 215, Loss: 0.11873167008161545\n",
      "Epoch: 0, Batch: 216, Loss: 0.1878850758075714\n",
      "Epoch: 0, Batch: 217, Loss: 0.10760282725095749\n",
      "Epoch: 0, Batch: 218, Loss: 0.1069720983505249\n",
      "Epoch: 0, Batch: 219, Loss: 0.10635334253311157\n",
      "Epoch: 0, Batch: 220, Loss: 0.11920066177845001\n",
      "Epoch: 0, Batch: 221, Loss: 0.19757400453090668\n",
      "Epoch: 0, Batch: 222, Loss: 0.11612549424171448\n",
      "Epoch: 0, Batch: 223, Loss: 0.10725314170122147\n",
      "Epoch: 0, Batch: 224, Loss: 0.10284949094057083\n",
      "Epoch: 0, Batch: 225, Loss: 0.1319517344236374\n",
      "Epoch: 0, Batch: 226, Loss: 0.13298508524894714\n",
      "Epoch: 0, Batch: 227, Loss: 0.11085209250450134\n",
      "Epoch: 0, Batch: 228, Loss: 0.19856111705303192\n",
      "Epoch: 0, Batch: 229, Loss: 0.09895903617143631\n",
      "Epoch: 0, Batch: 230, Loss: 0.1077447161078453\n",
      "Epoch: 0, Batch: 231, Loss: 0.12296987324953079\n",
      "Epoch: 0, Batch: 232, Loss: 0.09790924936532974\n",
      "Epoch: 0, Batch: 233, Loss: 0.12473142147064209\n",
      "Epoch: 0, Batch: 234, Loss: 0.11012928932905197\n",
      "Epoch: 0, Batch: 235, Loss: 0.308394193649292\n",
      "Epoch: 0, Batch: 236, Loss: 0.11556259542703629\n",
      "Epoch: 0, Batch: 237, Loss: 0.0972491130232811\n",
      "Epoch: 0, Batch: 238, Loss: 0.390586793422699\n",
      "Epoch: 0, Batch: 239, Loss: 0.1002778708934784\n",
      "Epoch: 0, Batch: 240, Loss: 0.17993997037410736\n",
      "Epoch: 0, Batch: 241, Loss: 0.10382352024316788\n",
      "Epoch: 0, Batch: 242, Loss: 0.09899529814720154\n",
      "Epoch: 0, Batch: 243, Loss: 0.15357132256031036\n",
      "Epoch: 0, Batch: 244, Loss: 0.12411481887102127\n",
      "Epoch: 0, Batch: 245, Loss: 0.09137868881225586\n",
      "Epoch: 0, Batch: 246, Loss: 0.08959081768989563\n",
      "Epoch: 0, Batch: 247, Loss: 0.28936338424682617\n",
      "Epoch: 0, Batch: 248, Loss: 0.08867919445037842\n",
      "Epoch: 0, Batch: 249, Loss: 0.09557589888572693\n",
      "Epoch: 0, Batch: 250, Loss: 0.09863797575235367\n",
      "Epoch: 0, Batch: 251, Loss: 0.09092843532562256\n",
      "Epoch: 0, Batch: 252, Loss: 0.08655858039855957\n",
      "Epoch: 0, Batch: 253, Loss: 0.0855349600315094\n",
      "Epoch: 0, Batch: 254, Loss: 0.09666649997234344\n",
      "Epoch: 0, Batch: 255, Loss: 0.27338099479675293\n",
      "Epoch: 0, Batch: 256, Loss: 0.08490704745054245\n",
      "Epoch: 0, Batch: 257, Loss: 0.11108138412237167\n",
      "Epoch: 0, Batch: 258, Loss: 0.08706316351890564\n",
      "Epoch: 0, Batch: 259, Loss: 0.0872945785522461\n",
      "Epoch: 0, Batch: 260, Loss: 0.08677779138088226\n",
      "Epoch: 0, Batch: 261, Loss: 0.08571929484605789\n",
      "Epoch: 0, Batch: 262, Loss: 0.08448612689971924\n",
      "Epoch: 0, Batch: 263, Loss: 0.11405559629201889\n",
      "Epoch: 0, Batch: 264, Loss: 0.08657818287611008\n",
      "Epoch: 0, Batch: 265, Loss: 0.3868723213672638\n",
      "Epoch: 0, Batch: 266, Loss: 0.1469920426607132\n",
      "Epoch: 0, Batch: 267, Loss: 0.08125327527523041\n",
      "Epoch: 0, Batch: 268, Loss: 0.10348526388406754\n",
      "Epoch: 0, Batch: 269, Loss: 0.09282419830560684\n",
      "Epoch: 0, Batch: 270, Loss: 0.24897918105125427\n",
      "Epoch: 0, Batch: 271, Loss: 0.24525922536849976\n",
      "Epoch: 0, Batch: 272, Loss: 0.0793493315577507\n",
      "Epoch: 0, Batch: 273, Loss: 0.1590917408466339\n",
      "Epoch: 0, Batch: 274, Loss: 0.10063478350639343\n",
      "Epoch: 0, Batch: 275, Loss: 0.14103521406650543\n",
      "Epoch: 0, Batch: 276, Loss: 0.07926061749458313\n",
      "Epoch: 0, Batch: 277, Loss: 0.31522491574287415\n",
      "Epoch: 0, Batch: 278, Loss: 0.08605632185935974\n",
      "Epoch: 0, Batch: 279, Loss: 0.08555108308792114\n",
      "Epoch: 0, Batch: 280, Loss: 0.07860525697469711\n",
      "Epoch: 0, Batch: 281, Loss: 0.09430835396051407\n",
      "Epoch: 0, Batch: 282, Loss: 0.0843115746974945\n",
      "Epoch: 0, Batch: 283, Loss: 0.07929468899965286\n",
      "Epoch: 0, Batch: 284, Loss: 0.08149930089712143\n",
      "Epoch: 0, Batch: 285, Loss: 0.08397474139928818\n",
      "Epoch: 0, Batch: 286, Loss: 0.07616399228572845\n",
      "Epoch: 0, Batch: 287, Loss: 0.07149961590766907\n",
      "Epoch: 0, Batch: 288, Loss: 0.07426167279481888\n",
      "Epoch: 0, Batch: 289, Loss: 0.07201409339904785\n",
      "Epoch: 0, Batch: 290, Loss: 0.06903036683797836\n",
      "Epoch: 0, Batch: 291, Loss: 0.0722508579492569\n",
      "Epoch: 0, Batch: 292, Loss: 0.07830367237329483\n",
      "Epoch: 0, Batch: 293, Loss: 0.11195442825555801\n",
      "Epoch: 0, Batch: 294, Loss: 0.0670083612203598\n",
      "Epoch: 0, Batch: 295, Loss: 0.09023220837116241\n",
      "Epoch: 0, Batch: 296, Loss: 0.09038068354129791\n",
      "Epoch: 0, Batch: 297, Loss: 0.0689752921462059\n",
      "Epoch: 0, Batch: 298, Loss: 0.06527582556009293\n",
      "Epoch: 0, Batch: 299, Loss: 0.06481729447841644\n",
      "Epoch: 0, Batch: 300, Loss: 0.1264204978942871\n",
      "Epoch: 0, Batch: 301, Loss: 0.06392837315797806\n",
      "Epoch: 0, Batch: 302, Loss: 0.1661510020494461\n",
      "Epoch: 0, Batch: 303, Loss: 0.06307315826416016\n",
      "Epoch: 0, Batch: 304, Loss: 0.06263823062181473\n",
      "Epoch: 0, Batch: 305, Loss: 0.07906924188137054\n",
      "Epoch: 0, Batch: 306, Loss: 0.16229292750358582\n",
      "Epoch: 0, Batch: 307, Loss: 0.06162577494978905\n",
      "Epoch: 0, Batch: 308, Loss: 0.06543473899364471\n",
      "Epoch: 0, Batch: 309, Loss: 0.07092992216348648\n",
      "Epoch: 0, Batch: 310, Loss: 0.06562037020921707\n",
      "Epoch: 0, Batch: 311, Loss: 0.11331577599048615\n",
      "Epoch: 0, Batch: 312, Loss: 0.06425879895687103\n",
      "Epoch: 0, Batch: 313, Loss: 0.1343417912721634\n",
      "Epoch: 0, Batch: 314, Loss: 0.05966997891664505\n",
      "Epoch: 0, Batch: 315, Loss: 0.05938014015555382\n",
      "Epoch: 0, Batch: 316, Loss: 0.059069789946079254\n",
      "Epoch: 0, Batch: 317, Loss: 0.11118198931217194\n",
      "Epoch: 0, Batch: 318, Loss: 0.07587334513664246\n",
      "Epoch: 0, Batch: 319, Loss: 0.07393067330121994\n",
      "Epoch: 0, Batch: 320, Loss: 0.2852572798728943\n",
      "Epoch: 0, Batch: 321, Loss: 0.08043646812438965\n",
      "Epoch: 0, Batch: 322, Loss: 0.11355181783437729\n",
      "Epoch: 0, Batch: 323, Loss: 0.06869302690029144\n",
      "Epoch: 0, Batch: 324, Loss: 0.13132837414741516\n",
      "Epoch: 0, Batch: 325, Loss: 0.06359836459159851\n",
      "Epoch: 0, Batch: 326, Loss: 0.06625092774629593\n",
      "Epoch: 0, Batch: 327, Loss: 0.14674849808216095\n",
      "Epoch: 0, Batch: 328, Loss: 0.0602559931576252\n",
      "Epoch: 0, Batch: 329, Loss: 0.06017635017633438\n",
      "Epoch: 0, Batch: 330, Loss: 0.06882301717996597\n",
      "Epoch: 0, Batch: 331, Loss: 0.08092943578958511\n",
      "Epoch: 0, Batch: 332, Loss: 0.060311321169137955\n",
      "Epoch: 0, Batch: 333, Loss: 0.11985892802476883\n",
      "Epoch: 0, Batch: 334, Loss: 0.07368134707212448\n",
      "Epoch: 0, Batch: 335, Loss: 0.0550057552754879\n",
      "Epoch: 0, Batch: 336, Loss: 0.05958276987075806\n",
      "Epoch: 0, Batch: 337, Loss: 0.06943371891975403\n",
      "Epoch: 0, Batch: 338, Loss: 0.06897728145122528\n",
      "Epoch: 0, Batch: 339, Loss: 0.05233558639883995\n",
      "Epoch: 0, Batch: 340, Loss: 0.12314033508300781\n",
      "Epoch: 0, Batch: 341, Loss: 0.582922101020813\n",
      "Epoch: 0, Batch: 342, Loss: 0.26432913541793823\n",
      "Epoch: 0, Batch: 343, Loss: 0.05581212043762207\n",
      "Epoch: 0, Batch: 344, Loss: 0.08316528797149658\n",
      "Epoch: 0, Batch: 345, Loss: 0.054801974445581436\n",
      "Epoch: 0, Batch: 346, Loss: 0.23263652622699738\n",
      "Epoch: 0, Batch: 347, Loss: 0.05710415169596672\n",
      "Epoch: 0, Batch: 348, Loss: 0.06625515222549438\n",
      "Epoch: 0, Batch: 349, Loss: 0.06163026764988899\n",
      "Epoch: 0, Batch: 350, Loss: 0.12257111817598343\n",
      "Epoch: 0, Batch: 351, Loss: 0.061696745455265045\n",
      "Epoch: 0, Batch: 352, Loss: 0.058400291949510574\n",
      "Epoch: 0, Batch: 353, Loss: 0.057564783841371536\n",
      "Epoch: 0, Batch: 354, Loss: 0.15063799917697906\n",
      "Epoch: 0, Batch: 355, Loss: 0.06706598401069641\n",
      "Epoch: 0, Batch: 356, Loss: 0.054751183837652206\n",
      "Epoch: 0, Batch: 357, Loss: 0.053689830005168915\n",
      "Epoch: 0, Batch: 358, Loss: 0.05261944606900215\n",
      "Epoch: 0, Batch: 359, Loss: 0.13331834971904755\n",
      "Epoch: 0, Batch: 360, Loss: 0.05377562344074249\n",
      "Epoch: 0, Batch: 361, Loss: 0.08176851272583008\n",
      "Epoch: 0, Batch: 362, Loss: 0.054419826716184616\n",
      "Epoch: 0, Batch: 363, Loss: 0.091176338493824\n",
      "Epoch: 0, Batch: 364, Loss: 0.047852542251348495\n",
      "Epoch: 0, Batch: 365, Loss: 0.06141083315014839\n",
      "Epoch: 0, Batch: 366, Loss: 0.16856980323791504\n",
      "Epoch: 0, Batch: 367, Loss: 0.058123841881752014\n",
      "Epoch: 0, Batch: 368, Loss: 0.04622606560587883\n",
      "Epoch: 0, Batch: 369, Loss: 0.22726845741271973\n",
      "Epoch: 0, Batch: 370, Loss: 0.05932139232754707\n",
      "Epoch: 0, Batch: 371, Loss: 0.04577229171991348\n",
      "Epoch: 0, Batch: 372, Loss: 0.13626988232135773\n",
      "Epoch: 0, Batch: 373, Loss: 0.04544268548488617\n",
      "Epoch: 0, Batch: 374, Loss: 0.05586744472384453\n",
      "Epoch: 0, Batch: 375, Loss: 0.054049063473939896\n",
      "Epoch: 0, Batch: 376, Loss: 0.06242060661315918\n",
      "Epoch: 0, Batch: 377, Loss: 0.05987199768424034\n",
      "Epoch: 0, Batch: 378, Loss: 0.04927380010485649\n",
      "Epoch: 0, Batch: 379, Loss: 0.06210751086473465\n",
      "Epoch: 0, Batch: 380, Loss: 0.27054738998413086\n",
      "Epoch: 0, Batch: 381, Loss: 0.044003430753946304\n",
      "Epoch: 0, Batch: 382, Loss: 0.061325833201408386\n",
      "Epoch: 0, Batch: 383, Loss: 0.044354088604450226\n",
      "Epoch: 0, Batch: 384, Loss: 0.175482839345932\n",
      "Epoch: 0, Batch: 385, Loss: 0.1459355652332306\n",
      "Epoch: 0, Batch: 386, Loss: 0.0454825721681118\n",
      "Epoch: 0, Batch: 387, Loss: 0.16172270476818085\n",
      "Epoch: 0, Batch: 388, Loss: 0.04670341685414314\n",
      "Epoch: 0, Batch: 389, Loss: 0.17431631684303284\n",
      "Epoch: 0, Batch: 390, Loss: 0.10185614973306656\n",
      "Epoch: 0, Batch: 391, Loss: 0.062178559601306915\n",
      "Epoch: 0, Batch: 392, Loss: 0.05792154371738434\n",
      "Epoch: 0, Batch: 393, Loss: 0.07131327688694\n",
      "Epoch: 0, Batch: 394, Loss: 0.06685983389616013\n",
      "Epoch: 0, Batch: 395, Loss: 0.04802428185939789\n",
      "Epoch: 0, Batch: 396, Loss: 0.04663277789950371\n",
      "Epoch: 0, Batch: 397, Loss: 0.07270865887403488\n",
      "Epoch: 0, Batch: 398, Loss: 0.04866577312350273\n",
      "Epoch: 0, Batch: 399, Loss: 0.05652201548218727\n",
      "Epoch: 0, Batch: 400, Loss: 0.0529174730181694\n",
      "Epoch: 0, Batch: 401, Loss: 0.19789429008960724\n",
      "Epoch: 0, Batch: 402, Loss: 0.043091874569654465\n",
      "Epoch: 0, Batch: 403, Loss: 0.05766473338007927\n",
      "Epoch: 0, Batch: 404, Loss: 0.04505249485373497\n",
      "Epoch: 0, Batch: 405, Loss: 0.047796688973903656\n",
      "Epoch: 0, Batch: 406, Loss: 0.04405737668275833\n",
      "Epoch: 0, Batch: 407, Loss: 0.06469985097646713\n",
      "Epoch: 0, Batch: 408, Loss: 0.058624327182769775\n",
      "Epoch: 0, Batch: 409, Loss: 0.0571390725672245\n",
      "Epoch: 0, Batch: 410, Loss: 0.03964508697390556\n",
      "Epoch: 0, Batch: 411, Loss: 0.039196453988552094\n",
      "Epoch: 0, Batch: 412, Loss: 0.05653813108801842\n",
      "Epoch: 0, Batch: 413, Loss: 0.051023274660110474\n",
      "Epoch: 0, Batch: 414, Loss: 0.04877413064241409\n",
      "Epoch: 0, Batch: 415, Loss: 0.03758442774415016\n",
      "Epoch: 0, Batch: 416, Loss: 0.046687569469213486\n",
      "Epoch: 0, Batch: 417, Loss: 0.036914147436618805\n",
      "Epoch: 0, Batch: 418, Loss: 0.03660263493657112\n",
      "Epoch: 0, Batch: 419, Loss: 0.036313630640506744\n",
      "Epoch: 0, Batch: 420, Loss: 0.03604511916637421\n",
      "Epoch: 0, Batch: 421, Loss: 0.0627073124051094\n",
      "Epoch: 0, Batch: 422, Loss: 0.15609005093574524\n",
      "Epoch: 0, Batch: 423, Loss: 0.03539709001779556\n",
      "Epoch: 0, Batch: 424, Loss: 0.03522124141454697\n",
      "Epoch: 0, Batch: 425, Loss: 0.03997565433382988\n",
      "Epoch: 0, Batch: 426, Loss: 0.04086623713374138\n",
      "Epoch: 0, Batch: 427, Loss: 0.03749235346913338\n",
      "Epoch: 0, Batch: 428, Loss: 0.0344868004322052\n",
      "Epoch: 0, Batch: 429, Loss: 0.06251528859138489\n",
      "Epoch: 0, Batch: 430, Loss: 0.03411847725510597\n",
      "Epoch: 0, Batch: 431, Loss: 0.060855433344841\n",
      "Epoch: 0, Batch: 432, Loss: 0.038680434226989746\n",
      "Epoch: 0, Batch: 433, Loss: 0.03762304410338402\n",
      "Epoch: 0, Batch: 434, Loss: 0.05031236261129379\n",
      "Epoch: 0, Batch: 435, Loss: 0.17101669311523438\n",
      "Epoch: 0, Batch: 436, Loss: 0.09928944706916809\n",
      "Epoch: 0, Batch: 437, Loss: 0.10587813705205917\n",
      "Epoch: 0, Batch: 438, Loss: 0.03952837362885475\n",
      "Epoch: 0, Batch: 439, Loss: 0.03729846701025963\n",
      "Epoch: 0, Batch: 440, Loss: 0.033420778810977936\n",
      "Epoch: 0, Batch: 441, Loss: 0.2997741401195526\n",
      "Epoch: 0, Batch: 442, Loss: 0.033858560025691986\n",
      "Epoch: 0, Batch: 443, Loss: 0.2952060103416443\n",
      "Epoch: 0, Batch: 444, Loss: 0.2056020051240921\n",
      "Epoch: 0, Batch: 445, Loss: 0.03708722069859505\n",
      "Epoch: 0, Batch: 446, Loss: 0.047410037368535995\n",
      "Epoch: 0, Batch: 447, Loss: 0.039503853768110275\n",
      "Epoch: 0, Batch: 448, Loss: 0.03981836885213852\n",
      "Epoch: 0, Batch: 449, Loss: 0.0999973863363266\n",
      "Epoch: 0, Batch: 450, Loss: 0.06607614457607269\n",
      "Epoch: 0, Batch: 451, Loss: 0.03869533911347389\n",
      "Epoch: 0, Batch: 452, Loss: 0.4180906116962433\n",
      "Epoch: 0, Batch: 453, Loss: 0.10378788411617279\n",
      "Epoch: 0, Batch: 454, Loss: 0.04043072834610939\n",
      "Epoch: 0, Batch: 455, Loss: 0.04336858168244362\n",
      "Epoch: 0, Batch: 456, Loss: 0.05250921472907066\n",
      "Epoch: 0, Batch: 457, Loss: 0.0414007343351841\n",
      "Epoch: 0, Batch: 458, Loss: 0.04095017910003662\n",
      "Epoch: 0, Batch: 459, Loss: 0.040198914706707\n",
      "Epoch: 0, Batch: 460, Loss: 0.09241296350955963\n",
      "Epoch: 0, Batch: 461, Loss: 0.07898682355880737\n",
      "Epoch: 0, Batch: 462, Loss: 0.03763102740049362\n",
      "Epoch: 0, Batch: 463, Loss: 0.036772921681404114\n",
      "Epoch: 0, Batch: 464, Loss: 0.03588061407208443\n",
      "Epoch: 0, Batch: 465, Loss: 0.0680588036775589\n",
      "Epoch: 0, Batch: 466, Loss: 0.05696742236614227\n",
      "Epoch: 0, Batch: 467, Loss: 0.05411195009946823\n",
      "Epoch: 0, Batch: 468, Loss: 0.050059255212545395\n",
      "Epoch: 0, Batch: 469, Loss: 0.19359342753887177\n",
      "Epoch: 0, Batch: 470, Loss: 0.031917307525873184\n",
      "Epoch: 0, Batch: 471, Loss: 0.03869093582034111\n",
      "Epoch: 0, Batch: 472, Loss: 0.03942714259028435\n",
      "Epoch: 0, Batch: 473, Loss: 0.03079003468155861\n",
      "Epoch: 0, Batch: 474, Loss: 0.045164041221141815\n",
      "Epoch: 0, Batch: 475, Loss: 0.030101925134658813\n",
      "Epoch: 0, Batch: 476, Loss: 0.029799336567521095\n",
      "Epoch: 0, Batch: 477, Loss: 0.03952953591942787\n",
      "Epoch: 0, Batch: 478, Loss: 0.04649177938699722\n",
      "Epoch: 0, Batch: 479, Loss: 0.02917557954788208\n",
      "Epoch: 0, Batch: 480, Loss: 0.03708001971244812\n",
      "Epoch: 0, Batch: 481, Loss: 0.02884986624121666\n",
      "Epoch: 0, Batch: 482, Loss: 0.028671836480498314\n",
      "Epoch: 0, Batch: 483, Loss: 0.14817428588867188\n",
      "Epoch: 0, Batch: 484, Loss: 0.15363310277462006\n",
      "Epoch: 0, Batch: 485, Loss: 0.02833140641450882\n",
      "Epoch: 0, Batch: 486, Loss: 0.028319254517555237\n",
      "Epoch: 0, Batch: 487, Loss: 0.02829478308558464\n",
      "Epoch: 0, Batch: 488, Loss: 0.03796346113085747\n",
      "Epoch: 0, Batch: 489, Loss: 0.03989666327834129\n",
      "Epoch: 0, Batch: 490, Loss: 0.028068214654922485\n",
      "Epoch: 0, Batch: 491, Loss: 0.04107091575860977\n",
      "Epoch: 0, Batch: 492, Loss: 0.04191922768950462\n",
      "Epoch: 0, Batch: 493, Loss: 0.027782397344708443\n",
      "Epoch: 0, Batch: 494, Loss: 0.0625636875629425\n",
      "Epoch: 0, Batch: 495, Loss: 0.0514691099524498\n",
      "Epoch: 0, Batch: 496, Loss: 0.11910650879144669\n",
      "Epoch: 0, Batch: 497, Loss: 0.04081416130065918\n",
      "Epoch: 0, Batch: 498, Loss: 0.030506877228617668\n",
      "Epoch: 0, Batch: 499, Loss: 0.06917914748191833\n",
      "Epoch: 0, Batch: 500, Loss: 0.0375831313431263\n",
      "Epoch: 0, Batch: 501, Loss: 0.027251726016402245\n",
      "Epoch: 0, Batch: 502, Loss: 0.32265111804008484\n",
      "Epoch: 0, Batch: 503, Loss: 0.02918841503560543\n",
      "Epoch: 0, Batch: 504, Loss: 0.12441961467266083\n",
      "Epoch: 0, Batch: 505, Loss: 0.035828232765197754\n",
      "Epoch: 0, Batch: 506, Loss: 0.0390479601919651\n",
      "Epoch: 0, Batch: 507, Loss: 0.03692597895860672\n",
      "Epoch: 0, Batch: 508, Loss: 0.028291970491409302\n",
      "Epoch: 0, Batch: 509, Loss: 0.36815646290779114\n",
      "Epoch: 0, Batch: 510, Loss: 0.04024990648031235\n",
      "Epoch: 0, Batch: 511, Loss: 0.029587993398308754\n",
      "Epoch: 0, Batch: 512, Loss: 0.030039001256227493\n",
      "Epoch: 0, Batch: 513, Loss: 0.03764893487095833\n",
      "Epoch: 0, Batch: 514, Loss: 0.05504244565963745\n",
      "Epoch: 0, Batch: 515, Loss: 0.06874214112758636\n",
      "Epoch: 0, Batch: 516, Loss: 0.042950306087732315\n",
      "Epoch: 0, Batch: 517, Loss: 0.03718538582324982\n",
      "Epoch: 0, Batch: 518, Loss: 0.029836244881153107\n",
      "Epoch: 0, Batch: 519, Loss: 0.05095943063497543\n",
      "Epoch: 0, Batch: 520, Loss: 0.06501426547765732\n",
      "Epoch: 0, Batch: 521, Loss: 0.03914010152220726\n",
      "Epoch: 0, Batch: 522, Loss: 0.027787799015641212\n",
      "Epoch: 0, Batch: 523, Loss: 0.02694288268685341\n",
      "Epoch: 0, Batch: 524, Loss: 0.04279455915093422\n",
      "Epoch: 0, Batch: 525, Loss: 0.03708961606025696\n",
      "Epoch: 0, Batch: 526, Loss: 0.12505964934825897\n",
      "Epoch: 0, Batch: 527, Loss: 0.023607980459928513\n",
      "Epoch: 0, Batch: 528, Loss: 0.11305481940507889\n",
      "Epoch: 0, Batch: 529, Loss: 0.023351825773715973\n",
      "Epoch: 0, Batch: 530, Loss: 0.04788665100932121\n",
      "Epoch: 0, Batch: 531, Loss: 0.023046022281050682\n",
      "Epoch: 0, Batch: 532, Loss: 0.022799372673034668\n",
      "Epoch: 0, Batch: 533, Loss: 0.022498711943626404\n",
      "Epoch: 0, Batch: 534, Loss: 0.038950178772211075\n",
      "Epoch: 0, Batch: 535, Loss: 0.1806246042251587\n",
      "Epoch: 0, Batch: 536, Loss: 0.07401307672262192\n",
      "Epoch: 0, Batch: 537, Loss: 0.046970613300800323\n",
      "Epoch: 0, Batch: 538, Loss: 0.20658928155899048\n",
      "Epoch: 0, Batch: 539, Loss: 0.027688950300216675\n",
      "Epoch: 0, Batch: 540, Loss: 0.022008968517184258\n",
      "Epoch: 0, Batch: 541, Loss: 0.03179597109556198\n",
      "Epoch: 0, Batch: 542, Loss: 0.04640892893075943\n",
      "Epoch: 0, Batch: 543, Loss: 0.03490591049194336\n",
      "Epoch: 0, Batch: 544, Loss: 0.021762294694781303\n",
      "Epoch: 0, Batch: 545, Loss: 0.03204315900802612\n",
      "Epoch: 0, Batch: 546, Loss: 0.20460541546344757\n",
      "Epoch: 0, Batch: 547, Loss: 0.027094393968582153\n",
      "Epoch: 0, Batch: 548, Loss: 0.047061044722795486\n",
      "Epoch: 0, Batch: 549, Loss: 0.029246803373098373\n",
      "Epoch: 0, Batch: 550, Loss: 0.04118451476097107\n",
      "Epoch: 0, Batch: 551, Loss: 0.04686128348112106\n",
      "Epoch: 0, Batch: 552, Loss: 0.02092297002673149\n",
      "Epoch: 0, Batch: 553, Loss: 0.2764385938644409\n",
      "Epoch: 0, Batch: 554, Loss: 0.09922859072685242\n",
      "Epoch: 0, Batch: 555, Loss: 0.03842705860733986\n",
      "Epoch: 0, Batch: 556, Loss: 0.0345606692135334\n",
      "Epoch: 0, Batch: 557, Loss: 0.02777535654604435\n",
      "Epoch: 0, Batch: 558, Loss: 0.02160542644560337\n",
      "Epoch: 0, Batch: 559, Loss: 0.02720138430595398\n",
      "Epoch: 0, Batch: 560, Loss: 0.030666828155517578\n",
      "Epoch: 0, Batch: 561, Loss: 0.16350625455379486\n",
      "Epoch: 0, Batch: 562, Loss: 0.02176828496158123\n",
      "Epoch: 0, Batch: 563, Loss: 0.02180534601211548\n",
      "Epoch: 0, Batch: 564, Loss: 0.021788181737065315\n",
      "Epoch: 0, Batch: 565, Loss: 0.048595450818538666\n",
      "Epoch: 0, Batch: 566, Loss: 0.05746396631002426\n",
      "Epoch: 0, Batch: 567, Loss: 0.021584726870059967\n",
      "Epoch: 0, Batch: 568, Loss: 0.04260239005088806\n",
      "Epoch: 0, Batch: 569, Loss: 0.0366210862994194\n",
      "Epoch: 0, Batch: 570, Loss: 0.021222952753305435\n",
      "Epoch: 0, Batch: 571, Loss: 0.02105223573744297\n",
      "Epoch: 0, Batch: 572, Loss: 0.038351185619831085\n",
      "Epoch: 0, Batch: 573, Loss: 0.026226812973618507\n",
      "Epoch: 0, Batch: 574, Loss: 0.08753716945648193\n",
      "Epoch: 0, Batch: 575, Loss: 0.06331581622362137\n",
      "Epoch: 0, Batch: 576, Loss: 0.09724432229995728\n",
      "Epoch: 0, Batch: 577, Loss: 0.029200997203588486\n",
      "Epoch: 0, Batch: 578, Loss: 0.02017984725534916\n",
      "Epoch: 0, Batch: 579, Loss: 0.02823024056851864\n",
      "Epoch: 0, Batch: 580, Loss: 0.01996578648686409\n",
      "Epoch: 0, Batch: 581, Loss: 0.019811779260635376\n",
      "Epoch: 0, Batch: 582, Loss: 0.034431129693984985\n",
      "Epoch: 0, Batch: 583, Loss: 0.01951410248875618\n",
      "Epoch: 0, Batch: 584, Loss: 0.028325006365776062\n",
      "Epoch: 0, Batch: 585, Loss: 0.02981461025774479\n",
      "Epoch: 0, Batch: 586, Loss: 0.03532597795128822\n",
      "Epoch: 0, Batch: 587, Loss: 0.018886853009462357\n",
      "Epoch: 0, Batch: 588, Loss: 0.027916399762034416\n",
      "Epoch: 0, Batch: 589, Loss: 0.018577324226498604\n",
      "Epoch: 0, Batch: 590, Loss: 0.01842275634407997\n",
      "Epoch: 0, Batch: 591, Loss: 0.018270909786224365\n",
      "Epoch: 0, Batch: 592, Loss: 0.01812296360731125\n",
      "Epoch: 0, Batch: 593, Loss: 0.10469251126050949\n",
      "Epoch: 0, Batch: 594, Loss: 0.017888935282826424\n"
     ]
    }
   ],
   "source": [
    "# Train loop\n",
    "\n",
    "import wandb\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "luna_test_loader = DataLoader(Luna16Dataset(root=PROJECT_ROOT / \"data\" / \"luna16\", transforms=transforms, train=False), batch_size=1, shuffle=False)\n",
    "\n",
    "# run = None\n",
    "run = wandb.init(\n",
    "    project=\"luna16\",\n",
    "    group=None,\n",
    "    job_type=\"train\",\n",
    "    config={\n",
    "        \"model\": \"UNet3D\",\n",
    "        \"optimizer\": optimizer.__class__.__name__,\n",
    "        \"scheduler\": scheduler.__class__.__name__,\n",
    "        \"lr\": 0.001,\n",
    "        \"batch_size\": 1,\n",
    "        \"epochs\": 10,\n",
    "    }\n",
    ")\n",
    "\n",
    "model.train().to(device)\n",
    "for epoch in range(10):\n",
    "    for i, (data, target) in enumerate(luna_train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        target = target.float() # BCEWithLogitsLoss expects float targets\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Epoch: {epoch}, Batch: {i}, Loss: {loss.item()}\")\n",
    "        if run:\n",
    "            run.log({\"epoch\": epoch, \"batch\": i, \"train/loss\": loss.item()})\n",
    "    scheduler.step()\n",
    "    \n",
    "    losses = []\n",
    "    for i, (data, target) in enumerate(luna_test_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        target = target.float()\n",
    "        loss = criterion(output, target)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "    test_loss = sum(losses) / len(losses)  # Average loss over each image\n",
    "    print(f\"Epoch: {epoch}, Test loss: {test_loss}\")\n",
    "    if run:\n",
    "        run.log({\"epoch\": epoch, \"test/loss\": test_loss})\n",
    "\n",
    "if run:  \n",
    "    run.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4b55bcea492149",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
